{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download data yang dibutuhkan"
      ],
      "metadata": {
        "id": "tExnc9D8PiJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o ./question-pairs-dataset.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/quora/question-pairs-dataset\n",
        "!unzip question-pairs-dataset.zip -d ./\n",
        "!curl -L -o paws_wiki_labeled_final.tar.gz https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz\n",
        "!tar -xvzf paws_wiki_labeled_final.tar.gz"
      ],
      "metadata": {
        "id": "7y0I1RBMIA1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77da6b10-9d1f-4f38-e109-51a876db1d81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 20.7M  100 20.7M    0     0  5583k      0  0:00:03  0:00:03 --:--:-- 9617k\n",
            "Archive:  question-pairs-dataset.zip\n",
            "  inflating: ./questions.csv         \n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4577k  100 4577k    0     0  1683k      0  0:00:02  0:00:02 --:--:-- 1684k\n",
            "final/test.tsv\n",
            "final/\n",
            "final/train.tsv\n",
            "final/dev.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "mb6Rv4y3Pnjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the PAWS-Wiki dataset\n",
        "paws_train = pd.read_csv('final/train.tsv', sep='\\t')\n",
        "paws_test = pd.read_csv('final/test.tsv', sep='\\t')\n",
        "paws_dev = pd.read_csv('final/dev.tsv', sep='\\t')\n",
        "\n",
        "# Load the another dataset\n",
        "quora_data = pd.read_csv('questions.csv')\n",
        "\n",
        "# Prep Functions\n",
        "def lower(data) :\n",
        "    data['sentence1'] = data['sentence1'].apply(lambda x: x.lower())\n",
        "    data['sentence2'] = data['sentence2'].apply(lambda x: x.lower())\n",
        "\n",
        "# PAWS Preprocessing\n",
        "lower(paws_train)\n",
        "lower(paws_test)\n",
        "lower(paws_dev)\n",
        "\n",
        "paws_train = paws_train.drop(columns=['id'])\n",
        "paws_test = paws_test.drop(columns=['id'])\n",
        "paws_dev = paws_dev.drop(columns=['id'])\n",
        "\n",
        "paws_train = paws_train.dropna()\n",
        "paws_test = paws_test.dropna()\n",
        "paws_dev = paws_dev.dropna()\n",
        "\n",
        "#Quora Preprocessing\n",
        "quora_data = quora_data.drop(columns=['qid1', 'qid2', 'id'])\n",
        "quora_data = quora_data.rename(columns={'question1': 'sentence1', 'question2': 'sentence2', 'is_duplicate' : 'label'})\n",
        "quora_data = quora_data.dropna()\n",
        "lower(quora_data)\n",
        "\n",
        "quora_train, quora_testdev = train_test_split(quora_data, test_size=0.24, random_state=42)\n",
        "quora_test, quora_dev = train_test_split(quora_testdev, test_size=0.5, random_state=42)\n",
        "\n",
        "# Combine the datasets\n",
        "train_df = shuffle(pd.concat([paws_train, quora_train], ignore_index=True), random_state=40)\n",
        "test_df = shuffle(pd.concat([paws_test, quora_test], ignore_index=True), random_state=40)\n",
        "dev_df = shuffle(pd.concat([paws_dev, quora_dev], ignore_index=True), random_state=40)"
      ],
      "metadata": {
        "id": "SnQ_Qm2yIIwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0299ead-e813-424c-fb92-fa3c1bc31140"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3b8f1570eb4b>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['sentence1'] = data['sentence1'].apply(lambda x: x.lower())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi-fungsi untuk mengubah data ke format yang cocok untuk training model\n",
        "\n",
        "1. Tokenizing\n",
        "*   Mengubah teks menjadi huruf kecil.\n",
        "*   Menghapus karakter non-alfanumerik dan tanda baca tertentu.\n",
        "*   Memecah teks menjadi token-token (kata-kata).\n",
        "2. Persiapan Data\n",
        "*   Membaca pasangan kalimat dari DataFrame.\n",
        "*   Menghasilkan pasangan token (input_tokens, target_tokens) untuk setiap baris.\n",
        "*   Menambahkan token khusus <sos> di awal target dan <eos> di akhir sebagai penanda awal dan akhir kalimat.\n",
        "3. Membuat Vocabulary\n",
        "*   Menghitung frekuensi token dari pasangan input atau target.\n",
        "*   Menambahkan token ke dalam vocab jika frekuensinya ≥ min_freq.\n",
        "*   Token spesial seperti <pad>, <unk>, <sos>, <eos> dimasukkan terlebih dahulu dengan indeks tetap.\n",
        "4. Encoding Token\n",
        "*   Mengubah token menjadi indeks berdasarkan vocab.\n",
        "*   Token yang tidak ditemukan akan dikodekan sebagai <unk>.\n",
        "5. Dataset PyTorch\n",
        "*   Kelas dataset kustom yang menyimpan pasangan (input, target) dalam bentuk indeks tensor.\n",
        "*   Digunakan oleh DataLoader untuk training dan evaluasi.\n",
        "6. Padding Batch\n",
        "*   Fungsi collate_fn khusus untuk memastikan setiap batch memiliki panjang urutan yang sama.\n",
        "*   Digunakan oleh DataLoader agar model bisa memproses batch dengan ukuran berbeda.\n",
        "7. Membuat DataLoader\n",
        "*   Mempersiapkan DataLoader untuk data latih (train_df), validasi (dev_df), dan uji (test_df).\n",
        "*   Mengembalikan tiga DataLoader dan dua kamus vocabulary (src_vocab, trg_vocab).\n"
      ],
      "metadata": {
        "id": "f6F4KNEPPqYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hAzEdpYbDOKc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize input text by converting to lowercase\n",
        "    and removing special characters\n",
        "    \"\"\"\n",
        "    text = str(text).lower()  # Ensure text is a string\n",
        "    text = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", text)\n",
        "    return text.strip().split()\n",
        "\n",
        "def prepare_data(df, input_col, target_col):\n",
        "    \"\"\"\n",
        "    Prepare data pairs from DataFrame\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame\n",
        "        input_col (str): Column name for input sequences\n",
        "        target_col (str): Column name for target sequences\n",
        "\n",
        "    Returns:\n",
        "        list: List of (input_tokens, target_tokens) pairs\n",
        "    \"\"\"\n",
        "    return [\n",
        "        (tokenize(row[input_col]), ['<sos>'] + tokenize(row[target_col]) + ['<eos>'])\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "def build_vocab(pairs, index, min_freq=2):\n",
        "    \"\"\"\n",
        "    Build vocabulary from token pairs\n",
        "\n",
        "    Args:\n",
        "        pairs (list): List of (input, target) token pairs\n",
        "        index (int): 0 for input, 1 for target\n",
        "        min_freq (int): Minimum frequency to include a token\n",
        "\n",
        "    Returns:\n",
        "        dict: Vocabulary mapping tokens to indices\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    for pair in pairs:\n",
        "        counter.update(pair[index])\n",
        "\n",
        "    # Start with special tokens\n",
        "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq and word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def encode(tokens, vocab):\n",
        "    \"\"\"\n",
        "    Encode tokens to their vocabulary indices\n",
        "\n",
        "    Args:\n",
        "        tokens (list): List of tokens\n",
        "        vocab (dict): Vocabulary mapping\n",
        "\n",
        "    Returns:\n",
        "        list: List of token indices\n",
        "    \"\"\"\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Sequence-to-Sequence tasks\n",
        "    \"\"\"\n",
        "    def __init__(self, pairs, src_vocab, trg_vocab):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pairs (list): List of (input, target) token pairs\n",
        "            src_vocab (dict): Source vocabulary\n",
        "            trg_vocab (dict): Target vocabulary\n",
        "        \"\"\"\n",
        "        self.pairs = pairs\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, trg = self.pairs[idx]\n",
        "        return (\n",
        "            torch.tensor(encode(src, self.src_vocab)),\n",
        "            torch.tensor(encode(trg, self.trg_vocab))\n",
        "        )\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for padding sequences in a batch\n",
        "\n",
        "    Args:\n",
        "        batch (list): List of (src_sequence, trg_sequence) pairs\n",
        "\n",
        "    Returns:\n",
        "        tuple: Padded source and target batches\n",
        "    \"\"\"\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "\n",
        "    # Pad sequences, using 0 (pad token index) for padding\n",
        "    src_batch = pad_sequence(src_batch, padding_value=0, batch_first=True)\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=0, batch_first=True)\n",
        "\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "def prepare_dataloaders(train_df, dev_df, test_df,\n",
        "                        input_col='sentence1',\n",
        "                        target_col='sentence2',\n",
        "                        batch_size=32):\n",
        "    \"\"\"\n",
        "    Prepare DataLoaders for train, dev, and test datasets\n",
        "\n",
        "    Args:\n",
        "        train_df (pd.DataFrame): Training data\n",
        "        dev_df (pd.DataFrame): Development data\n",
        "        test_df (pd.DataFrame): Test data\n",
        "        input_col (str): Column name for input sequences\n",
        "        target_col (str): Column name for target sequences\n",
        "        batch_size (int): Batch size for DataLoaders\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_loader, dev_loader, test_loader), source vocab, target vocab\n",
        "    \"\"\"\n",
        "    # Prepare token pairs\n",
        "    train_pairs = prepare_data(train_df, input_col, target_col)\n",
        "    dev_pairs = prepare_data(dev_df, input_col, target_col)\n",
        "    test_pairs = prepare_data(test_df, input_col, target_col)\n",
        "\n",
        "    # Build vocabularies\n",
        "    src_vocab = build_vocab(train_pairs, 0)\n",
        "    trg_vocab = build_vocab(train_pairs, 1)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = Seq2SeqDataset(train_pairs, src_vocab, trg_vocab)\n",
        "    dev_dataset = Seq2SeqDataset(dev_pairs, src_vocab, trg_vocab)\n",
        "    test_dataset = Seq2SeqDataset(test_pairs, src_vocab, trg_vocab)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    dev_loader = DataLoader(\n",
        "        dev_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, dev_loader, test_loader, src_vocab, trg_vocab\n",
        "\n",
        "train_loader, dev_loader, test_loader, src_vocab, trg_vocab = prepare_dataloaders(\n",
        "    train_df, dev_df, test_df,\n",
        "    input_col='sentence1',\n",
        "    target_col='sentence2'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Seq2Seq Model Architecture**\n",
        "Model ini terdiri dari 3 komponen utama: Encoder, Decoder, dan Seq2Seq wrapper.\n",
        "1. Encoder\n",
        "\n",
        "Fungsi :\n",
        "*   Mengubah urutan token input menjadi representasi tersembunyi (hidden states).\n",
        "*   Menggunakan Embedding Layer diikuti oleh LSTM.\n",
        "\n",
        "Alur :    \n",
        "*   src (input) → Embedding\n",
        "*   Embedding → LSTM\n",
        "*   Mengembalikan: hidden dan cell (untuk digunakan oleh decoder)\n",
        "\n",
        "2. Decoder\n",
        "\n",
        "Fungsi :    \n",
        "*   Menghasilkan token satu per satu berdasarkan output sebelumnya dan konteks dari encoder.\n",
        "\n",
        "Alur :    \n",
        "*   input token saat ini → Embedding\n",
        "*   Embedding + hidden, cell → LSTM\n",
        "*   LSTM output → Linear layer → Prediksi token\n",
        "*   Mengembalikan prediksi, dan hidden, cell baru\n",
        "\n",
        "3. Seq2Seq Wrapper\n",
        "\n",
        "Fungsi :    \n",
        "*   Mengintegrasikan Encoder dan Decoder.\n",
        "*   Mengatur proses decoding selama pelatihan dan inference.\n",
        "\n",
        "Alur :    \n",
        "*   Encoder memproses seluruh input src.\n",
        "*   Decoder menghasilkan output secara iteratif.\n",
        "*   Gunakan Teacher Forcing:\n",
        "  *   Dengan probabilitas teacher_forcing_ratio, gunakan token target yang benar sebagai input berikutnya.\n",
        "  *   Jika tidak, gunakan prediksi model sebelumnya.\n",
        "\n",
        "Output :    \n",
        "Tensor berisi seluruh prediksi token untuk urutan target.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VgjgqSUOXG21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nnFun\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # [batch, 1]\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        trg_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "3YkpsK7vEhRs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_DIM = len(src_vocab)\n",
        "OUTPUT_DIM = len(trg_vocab)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model setup\n",
        "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\n",
        "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "# Loss and Optimizer\n",
        "PAD_IDX = trg_vocab['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "l1n09fbAEs2E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "N_EPOCHS = 2\n",
        "CHECKPOINT_EVERY = 200  # langkah (steps)\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "step = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\", leave=False)\n",
        "\n",
        "    for src, trg in loop:\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        step += 1\n",
        "        if step % CHECKPOINT_EVERY == 0:\n",
        "            ckpt_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch{epoch+1}_step{step}.pt\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'step': step,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss.item(),\n",
        "            }, ckpt_path)\n",
        "            print(f\"Checkpoint disimpan: {ckpt_path}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {epoch_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN6CEV4hGGy_",
        "outputId": "4e75b200-ec5c-48bf-fec1-21c16b0bfd13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   2%|▏         | 200/11148 [01:59<10:39:54,  3.51s/it, loss=6.44]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   4%|▎         | 400/11148 [03:54<11:43:38,  3.93s/it, loss=6.12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   5%|▌         | 600/11148 [05:51<11:00:24,  3.76s/it, loss=6.25]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   7%|▋         | 800/11148 [07:43<7:49:19,  2.72s/it, loss=5.92]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   9%|▉         | 1000/11148 [09:46<15:32:00,  5.51s/it, loss=5.52]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step1000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  11%|█         | 1200/11148 [11:38<10:30:11,  3.80s/it, loss=6.21]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step1200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  13%|█▎        | 1400/11148 [13:34<9:28:40,  3.50s/it, loss=5.23]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step1400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  14%|█▍        | 1600/11148 [15:25<7:33:16,  2.85s/it, loss=5.67]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step1600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  16%|█▌        | 1800/11148 [17:17<9:30:14,  3.66s/it, loss=5.9]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step1800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  18%|█▊        | 2000/11148 [19:21<9:22:48,  3.69s/it, loss=5.49]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step2000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  20%|█▉        | 2200/11148 [21:17<10:14:24,  4.12s/it, loss=5.61]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step2200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  22%|██▏       | 2400/11148 [23:13<6:50:30,  2.82s/it, loss=5.73]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step2400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  23%|██▎       | 2600/11148 [25:08<6:10:45,  2.60s/it, loss=5.83]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step2600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  25%|██▌       | 2800/11148 [27:05<10:05:18,  4.35s/it, loss=5.59]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step2800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  27%|██▋       | 3000/11148 [28:56<6:29:08,  2.87s/it, loss=5.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step3000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  29%|██▊       | 3200/11148 [30:52<8:22:39,  3.79s/it, loss=5.21]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step3200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  30%|███       | 3400/11148 [32:49<8:05:31,  3.76s/it, loss=5.54]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step3400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  32%|███▏      | 3600/11148 [34:41<5:27:53,  2.61s/it, loss=5.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step3600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  34%|███▍      | 3800/11148 [36:42<11:11:45,  5.49s/it, loss=4.92]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step3800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  36%|███▌      | 4000/11148 [38:38<6:18:55,  3.18s/it, loss=5.41]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step4000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  38%|███▊      | 4200/11148 [40:29<5:57:33,  3.09s/it, loss=5.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step4200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  39%|███▉      | 4400/11148 [42:25<7:09:24,  3.82s/it, loss=4.9]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step4400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  41%|████▏     | 4600/11148 [44:15<6:45:40,  3.72s/it, loss=4.94]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step4600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  43%|████▎     | 4800/11148 [46:11<6:32:02,  3.71s/it, loss=5.13]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step4800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  45%|████▍     | 5000/11148 [48:08<6:23:50,  3.75s/it, loss=5.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step5000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  47%|████▋     | 5200/11148 [50:05<6:33:00,  3.96s/it, loss=4.72]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step5200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  48%|████▊     | 5400/11148 [52:03<7:07:30,  4.46s/it, loss=4.93]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step5400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  50%|█████     | 5600/11148 [54:08<6:24:30,  4.16s/it, loss=5.03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step5600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  52%|█████▏    | 5800/11148 [56:04<4:27:39,  3.00s/it, loss=4.77]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step5800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  54%|█████▍    | 6000/11148 [58:04<5:58:20,  4.18s/it, loss=4.78]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step6000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  56%|█████▌    | 6200/11148 [1:00:09<6:36:11,  4.80s/it, loss=4.55]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step6200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  57%|█████▋    | 6400/11148 [1:02:07<5:39:41,  4.29s/it, loss=4.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step6400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  59%|█████▉    | 6600/11148 [1:04:04<4:37:19,  3.66s/it, loss=4.35]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step6600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  61%|██████    | 6800/11148 [1:06:02<4:34:52,  3.79s/it, loss=4.15]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step6800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  63%|██████▎   | 7000/11148 [1:08:07<4:11:55,  3.64s/it, loss=5.19]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step7000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  65%|██████▍   | 7200/11148 [1:10:07<5:23:25,  4.92s/it, loss=5.11]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step7200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  66%|██████▋   | 7400/11148 [1:12:01<2:31:46,  2.43s/it, loss=4.89]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step7400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  68%|██████▊   | 7600/11148 [1:14:05<3:58:08,  4.03s/it, loss=5.52]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step7600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  70%|██████▉   | 7800/11148 [1:16:03<3:46:00,  4.05s/it, loss=4.51]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step7800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  72%|███████▏  | 8000/11148 [1:17:53<1:13:47,  1.41s/it, loss=4.72]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step8000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  74%|███████▎  | 8200/11148 [1:19:44<1:15:35,  1.54s/it, loss=4.15]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step8200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  75%|███████▌  | 8400/11148 [1:21:52<1:59:45,  2.61s/it, loss=4.51]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step8400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  77%|███████▋  | 8600/11148 [1:23:44<1:40:29,  2.37s/it, loss=5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step8600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  79%|███████▉  | 8800/11148 [1:25:36<2:09:09,  3.30s/it, loss=4.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step8800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  81%|████████  | 9000/11148 [1:27:36<2:26:28,  4.09s/it, loss=4.3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step9000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  83%|████████▎ | 9200/11148 [1:29:36<2:28:21,  4.57s/it, loss=4.59]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step9200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  84%|████████▍ | 9400/11148 [1:31:24<1:26:47,  2.98s/it, loss=4.41]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step9400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  86%|████████▌ | 9600/11148 [1:33:22<2:10:40,  5.06s/it, loss=4.77]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step9600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  88%|████████▊ | 9800/11148 [1:35:35<1:25:59,  3.83s/it, loss=4.99]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step9800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  90%|████████▉ | 10000/11148 [1:37:23<31:34,  1.65s/it, loss=4.34]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step10000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  91%|█████████▏| 10200/11148 [1:39:22<26:03,  1.65s/it, loss=4.54]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step10200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  93%|█████████▎| 10400/11148 [1:41:17<19:06,  1.53s/it, loss=4.58]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step10400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  95%|█████████▌| 10600/11148 [1:43:10<23:05,  2.53s/it, loss=4.51]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step10600.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  97%|█████████▋| 10800/11148 [1:45:04<22:25,  3.87s/it, loss=4.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step10800.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  99%|█████████▊| 11000/11148 [1:47:01<05:34,  2.26s/it, loss=5.04]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch1_step11000.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Loss: 5.0701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2:   0%|          | 52/11148 [00:43<16:52:48,  5.48s/it, loss=4.66]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch2_step11200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2:   2%|▏         | 252/11148 [02:47<5:49:52,  1.93s/it, loss=4.43]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint disimpan: checkpoints/checkpoint_epoch2_step11400.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2:   4%|▍         | 436/11148 [04:18<1:28:25,  2.02it/s, loss=4.31]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, teacher_forcing_ratio=0)  # No teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Load a checkpoint (adjust file name as needed)\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"checkpoint_epoch1_step200.pt\")\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "test_loss = evaluate(model, test_loader, criterion, DEVICE)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "B4Jfg_Uf8yZm",
        "outputId": "2524f53e-dc55-4142-a965-6168eec03d26"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ca57b3c2f21e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {test_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-ca57b3c2f21e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# No teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b1b79bbecb18>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtrg_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    step = checkpoint['step']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"✅ Loaded checkpoint from epoch {epoch+1}, step {step}, loss: {loss:.4f}\")\n",
        "    return model, optimizer\n"
      ],
      "metadata": {
        "id": "qhsv7n2h58RP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion, device, max_batches=5):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    batches_evaluated = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in data_loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, teacher_forcing_ratio=0)  # no teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            total_loss += loss.item()\n",
        "            batches_evaluated += 1\n",
        "\n",
        "            if batches_evaluated >= max_batches:\n",
        "                break\n",
        "\n",
        "    return total_loss / batches_evaluated\n"
      ],
      "metadata": {
        "id": "zhCPQFEG6Buu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"checkpoints/checkpoint_epoch1_step200.pt\"\n",
        "\n",
        "model, optimizer = load_checkpoint(model, optimizer, checkpoint_path, DEVICE)\n",
        "dev_loss = evaluate(model, dev_loader, criterion, DEVICE)\n",
        "print(f\"Validation Loss: {dev_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJKa9s3E6EJ5",
        "outputId": "68e85dad-80c0-4113-c38b-b5d65a0b7ded"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded checkpoint from epoch 1, step 200, loss: 6.3531\n",
            "Validation Loss: 6.4478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Reverse trg_vocab to map indices back to tokens\n",
        "    idx2trg = {i: t for t, i in trg_vocab.items()}\n",
        "\n",
        "    # Tokenize and encode the input sentence\n",
        "    tokens = tokenize(sentence)\n",
        "    input_tensor = torch.tensor([encode(tokens, src_vocab)], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "        input_token = torch.tensor([trg_vocab['<sos>']], dtype=torch.long).to(device)\n",
        "        output_tokens = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "            top1 = output.argmax(1).item()\n",
        "            if top1 == trg_vocab['<eos>']:\n",
        "                break\n",
        "            output_tokens.append(idx2trg.get(top1, '<unk>'))\n",
        "            input_token = torch.tensor([top1], dtype=torch.long).to(device)\n",
        "\n",
        "    return ' '.join(output_tokens)\n"
      ],
      "metadata": {
        "id": "ZGt_-F5sIgCV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from a checkpoint\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"checkpoint_epoch1_step200.pt\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Use a sentence\n",
        "custom_sentence = \"How do i get?\"\n",
        "paraphrased = paraphrase_sentence(model, custom_sentence, src_vocab, trg_vocab, DEVICE)\n",
        "print(f\"Original: {custom_sentence}\")\n",
        "print(f\"Paraphrased: {paraphrased}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DsVVqYU6LBV",
        "outputId": "9f2d9f7a-8b72-4c69-fa4f-564d68d12178"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: How do i get?\n",
            "Paraphrased: what is the\n"
          ]
        }
      ]
    }
  ]
}